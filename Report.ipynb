{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Collaboration and Competition\" project report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the report for the \"Collaboration and Competition\" project (P3) in Deep Reinforcement Learning Nanodegree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, you will work with the Tennis environment.\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.4669857  -1.5\n",
      "  0.          0.         -6.83172083  6.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# load Tennis\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later deep reinforcement learning, \n",
    "\n",
    "- No. agents: 2\n",
    "- State space: 24\n",
    "- Action size: 2\n",
    "- Action type: continuous\n",
    "\n",
    "are particularly important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "            fc3_units (int): Number of nodes in the third hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture I used for this project is the same one from the project 2.  \n",
    "The Actor class defines actor network which is trained to output actions depending on states.  \n",
    "The Actor has three layers with fc1_units=128, fc2_units=128 and fc3_units are number of actions.  \n",
    "Meanwhile, Critic netwok defined by the Critic class learns to evaluate the actions given by the actor network.  \n",
    "The Critic also has three layers with fcs1_units=128, fc2_units=128 and fc3_unit is one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Actor and Critic networks defined above will be trained with the Agent class which works as a helper class interacting with the environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from model import Actor, Critic\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(3e4)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3       # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agents():\n",
    "    def __init__(self, state_size, action_size, random_seed, num_agents, train_mode, agent):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.num_agents = num_agents\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        self.agent = agent\n",
    "        self.agents = [self.agent(state_size, action_size, random_seed, num_agents, self.memory, idx, train_mode) for idx in range(self.num_agents)]\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones, timestep):\n",
    "        if self.train_mode:\n",
    "            rewards = np.reshape(rewards, self.num_agents, -1)\n",
    "            dones = np.reshape(dones, self.num_agents, -1)\n",
    "            self.memory.add(states, actions, rewards, next_states, dones)\n",
    "            if len(self.memory) > BATCH_SIZE and timestep % 1 == 0:\n",
    "                for _ in range(1):\n",
    "                    experiences = self.memory.sample()\n",
    "                    for idx, agent in enumerate(self.agents):\n",
    "                        agent.learn(experiences, GAMMA, idx)\n",
    "\n",
    "    def act(self, states, noise_weight=1.0, add_noise=True):\n",
    "        actions = []\n",
    "        for idx, agent in enumerate(self.agents):\n",
    "            if self.agent == SharedMemoryAgent:\n",
    "                action = agent.act(states[idx], noise_weight, add_noise)\n",
    "            elif self.agent == SharedStateAgent:\n",
    "                states = np.reshape(states, -1)\n",
    "                action = agent.act(states, noise_weight, add_noise)\n",
    "            actions.append(action)\n",
    "        return np.array(actions)\n",
    "\n",
    "    def reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.noise.reset()\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for agent in self.agents:\n",
    "            agent.soft_update(local_model, target_model, tau)\n",
    "    \n",
    "    def hard_update(self, target, source):\n",
    "        for agent in self.agents:\n",
    "            agent.hard_update(target, source)\n",
    "    \n",
    "    def noise_reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.noise.reset()\n",
    "\n",
    "class SharedMemoryAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, num_agents, Memory, id, train_mode):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.num_agents = num_agents\n",
    "        self.id = id\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        # Replay memory\n",
    "        self.memory = Memory\n",
    "    \n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "\n",
    "    def act(self, states, noise_weight=1.0, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        add_noise = self.train_mode\n",
    "        if add_noise:\n",
    "            self.noise_val = self.noise.sample() * noise_weight\n",
    "            action += self.noise_val\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma, i):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "            t: time step\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        agent_id = torch.tensor([i]).to(device)\n",
    "        states = states.reshape(-1, 2, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "        actions = actions.reshape(-1, 2, self.action_size).index_select(1, agent_id).squeeze(1)\n",
    "        rewards = rewards.reshape(-1, 2, 1).index_select(1, agent_id).squeeze(1)\n",
    "        dones = dones.reshape(-1, 2, 1).index_select(1, agent_id).squeeze(1)\n",
    "        next_states = next_states.reshape(-1, 2, self.state_size).index_select(1, agent_id).squeeze(1)\n",
    "        # print(states.shape, actions.shape, rewards.shape, dones.shape, next_states.shape)\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)   \n",
    "\n",
    "        # ----------------------- update noise ----------------------- #        \n",
    "        self.noise.reset()\n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class SharedStateAgent(SharedMemoryAgent):\n",
    "    def __init__(self, state_size, action_size, random_seed, num_agents, Memory, id, train_mode):\n",
    "        super().__init__(state_size, action_size, random_seed, num_agents, Memory, id, train_mode)\n",
    "        self.actor_local = Actor(state_size * self.num_agents, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size * self.num_agents, action_size, random_seed).to(device)\n",
    "        self.critic_local = Critic(state_size * self.num_agents, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size * self.num_agents, action_size, random_seed).to(device)\n",
    "\n",
    "    def learn(self, experiences, gamma, i,):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        agent_id = torch.tensor([i]).to(device)\n",
    "        states = states.reshape(-1, 1, self.state_size * self.num_agents).squeeze(1)\n",
    "        actions = actions.reshape(-1, 2, self.action_size).index_select(1, agent_id).squeeze(1)\n",
    "        rewards = rewards.reshape(-1, 2, 1).index_select(1, agent_id).squeeze(1)\n",
    "        dones = dones.reshape(-1, 2, 1).index_select(1, agent_id).squeeze(1)\n",
    "        next_states = next_states.reshape(-1, 1, self.state_size * self.num_agents).squeeze(1)\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)   \n",
    "\n",
    "        # ----------------------- update noise ----------------------- #        \n",
    "        self.noise.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent architecture is based on the one from the project 2 but made some major changes to introduce a concept of MADDPG.  \n",
    "\n",
    "so each agent still has the same methods from project 2, namely;\n",
    "- act: Output the actions based on the states (you can choose to add noise defined in OUNoise class)\n",
    "- step: Gather episode variables like state, action, next_step and add to memory buffer (see ReplayBuffer). If enough episodes are stored in the memory, update the network (learn)\n",
    "- learn: update using TD estimate for Critic and Monte Carlo for Actor networks\n",
    "- soft_update: soft update model parameters using θ_target = τ*θ_local + (1 - τ)*θ_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And new major changes include;\n",
    "\n",
    "- There is now Agents class that control multiple agents\n",
    "- One episode is quite short in this project so I disabled epsilon/decay of noise that I introduced in project 2\n",
    "- I wanted to introduce different levels of shared activities among agents. The most simple one, SharedMemoryAgent, is designed to share the same experiences with other agents\n",
    "- SharedMemoryAgent is initiated with 2 action size and 24 state size\n",
    "- In the meanwhile for SharedStateAgent, which is a child class of SharedMemoryAgent I like to loosen state observability condition a little bit and each actor and critic can observe states in the all agents (for now it doesn't really start learning well)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are as follows\n",
    "\n",
    "- BUFFER_SIZE: int(3e4)  # replay buffer size\n",
    "- BATCH_SIZE: 128        # minibatch size\n",
    "- GAMMA: 0.99            # discount factor\n",
    "- TAU: 1e-3              # for soft update of target parameters\n",
    "- LR_ACTOR: 1e-3         # learning rate of the actor \n",
    "- LR_CRITIC: 1e-3        # learning rate of the critic\n",
    "- WEIGHT_DECAY: 0        # L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual interaction with environment is done by the below helper function that has multiple for loops in it and updates networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from agents import *\n",
    "\n",
    "agent_names = {\n",
    "    'SharedMemoryAgent': SharedMemoryAgent,\n",
    "    'SharedStateAgent': SharedStateAgent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(n_episodes=4000, max_t=20000, print_every=100, train_mode=True, agent_name = 'SharedMemoryAgent'):\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = np.zeros(num_agents)\n",
    "    agent = Agents(\n",
    "        state_size=state_size, action_size=action_size, random_seed=2,\n",
    "        num_agents=num_agents, train_mode=train_mode, agent=agent_names[agent_name])\n",
    "    if train_mode == False:\n",
    "        for idx, a in enumerate(agent.agents):\n",
    "            a.actor_local.load_state_dict(torch.load('checkpoint_actor_{agent_str}_{idx}.pth'.format(agent_str=agent_name, idx=idx)))\n",
    "            a.critic_local.load_state_dict(torch.load('checkpoint_critic_{agent_str}_{idx}.pth'.format(agent_str=agent_name, idx=idx)))\n",
    "    scores_global = []\n",
    "    i_episode = 0\n",
    "    while True:\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "        states = env_info.vector_observations \n",
    "        agent.reset()\n",
    "        trewards = [[0] for _ in range(num_agents)]\n",
    "        agent.noise_reset()\n",
    "        for t in range(max_t):\n",
    "            if train_mode:\n",
    "                if i_episode < 100:\n",
    "                    actions = np.random.randn(num_agents, action_size)\n",
    "                    actions = np.clip(actions, -1, 1)  \n",
    "                else:\n",
    "                    actions = agent.act(states)\n",
    "            else:\n",
    "                actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones, t)\n",
    "            states = next_states \n",
    "            for i in range(agent.num_agents):\n",
    "                if rewards[i] != 0:\n",
    "                    trewards[i].append(rewards[i])\n",
    "            if (np.any(dones)):\n",
    "                break\n",
    "        i_episode += 1\n",
    "        per_agent_rewards = [sum(trewards[i]) for i in range(agent.num_agents)]\n",
    "        score = np.max(per_agent_rewards)\n",
    "        scores_global.append(score)\n",
    "        scores_deque.append(score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(\n",
    "            i_episode, avg_score, score), end=\"\")\n",
    "        if train_mode:\n",
    "            for idx, a in enumerate(agent.agents):\n",
    "                torch.save(a.actor_local.state_dict(), 'checkpoint_actor_{agent_str}_{idx}.pth'.format(agent_str=agent_name, idx=idx))\n",
    "                torch.save(a.critic_local.state_dict(), 'checkpoint_critic_{agent_str}_{idx}.pth'.format(agent_str=agent_name, idx=idx))\n",
    "            if avg_score > 0.5:\n",
    "                print(\"\\rsolved\")\n",
    "                for idx, a in enumerate(agent.agents):\n",
    "                    torch.save(a.actor_local.state_dict(), 'checkpoint_actor_{agent_str}_{idx}.pth'.format(agent_str=agent_name, idx=idx))\n",
    "                    torch.save(a.critic_local.state_dict(), 'checkpoint_critic_{agent_str}_{idx}.pth'.format(agent_str=agent_name, idx=idx))\n",
    "                return scores_global\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(\n",
    "            i_episode, avg_score, score))\n",
    "            \n",
    "    return scores_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runner function allows to do a basic reinforcement learning iteration as follows.  \n",
    "\n",
    "- The agent, state and score are initialized\n",
    "- for each episode\n",
    "- get state, action, reward, next_state\n",
    "- call step function (add the episode variables to memory and perform learning if enough data is stored in memory)\n",
    "- iterate this process and observe the score\n",
    "\n",
    "I realized positive actions are so sparse in this environment.  \n",
    "Just in order to get an initial learning on track at early stage, the agents take random actions till they experience 100 episodes. Random actions tend to lead to more positive results than actions given by early stage networks.  "
   ]
  },
  {
   "attachments": {
    "plot.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![plot.png](attachment:plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the score reached 0.5 at 1307th episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARL is an actively researched area so there are plenty of things I like to try\n",
    "- Introduce different levels of \"sharedness\" among agents like SharedStateAgent. In Tennis it would be natural to assume that the both agents can observe everything\n",
    "- Try to compare agents not only reacheability to 0.5 but highest scores or learning speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
